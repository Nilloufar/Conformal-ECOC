{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_classes(3) * n_clusters_per_class(1) must be smaller or equal 2**n_informative(1)=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 85\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[39mreturn\u001b[39;00m min_distances\n\u001b[1;32m     69\u001b[0m \u001b[39m# # Example usage\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m# X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# y_train = np.array([0, 1, 2, 1, 2, 0])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# X=df.iloc[:,:-1].values\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m# y=df.iloc[:,-1].values\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m X,y \u001b[39m=\u001b[39mget_X_y_data()\n\u001b[1;32m     86\u001b[0m \u001b[39m# X,y = [[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[2],[2],[2]],[1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2]\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m# X=np.array(X)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39m# y=np.array(y)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 6\u001b[0m, in \u001b[0;36mget_X_y_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_X_y_data\u001b[39m():\n\u001b[1;32m      5\u001b[0m \u001b[39m# Generate a synthetic classification dataset\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     X, y \u001b[39m=\u001b[39m make_classification(\n\u001b[1;32m      7\u001b[0m         n_samples\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,  \u001b[39m# Number of samples\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m     n_features\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m         n_informative\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[1;32m     10\u001b[0m         n_redundant\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m         n_classes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m         n_clusters_per_class\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m  \u001b[39m# Random seed for reproducibility\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[39m# Extract the first feature from the dataset\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     first_feature \u001b[39m=\u001b[39m X[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-MichiganStateUniversity/HAAL Lab/Projects/Conformed-ECOC/venv/lib/python3.11/site-packages/sklearn/datasets/_samples_generator.py:187\u001b[0m, in \u001b[0;36mmake_classification\u001b[0;34m(n_samples, n_features, n_informative, n_redundant, n_repeated, n_classes, n_clusters_per_class, weights, flip_y, class_sep, hypercube, shift, scale, shuffle, random_state)\u001b[0m\n\u001b[1;32m    185\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn_classes(\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) * n_clusters_per_class(\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m smaller or equal 2**n_informative(\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    188\u001b[0m         msg\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    189\u001b[0m             n_classes, n_clusters_per_class, n_informative, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mn_informative\n\u001b[1;32m    190\u001b[0m         )\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(weights) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [n_classes, n_classes \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]:\n",
      "\u001b[0;31mValueError\u001b[0m: n_classes(3) * n_clusters_per_class(1) must be smaller or equal 2**n_informative(1)=2"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import itertools\n",
    "from scipy.spatial import distance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TernaryECOC:\n",
    "    def __init__(self, n_classes, base_classifier=SVC(), n_classifiers=5):\n",
    "        self.base_classifier = base_classifier\n",
    "        self.classifiers = []\n",
    "        self.num_classifiers = n_classifiers\n",
    "        self.n_classes = n_classes\n",
    "        if n_classifiers != n_classes * (n_classes - 1) / 2:\n",
    "            raise ValueError(\"Number of classifiers must be equal to n_classes*(n_classes-1)/2\")\n",
    "        self.n_classifiers=n_classifiers\n",
    "        self.ecoc_matrix= self.generate_ovo_codes()\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for i in range(self.n_classifiers):\n",
    "            ternary_code = self.ecoc_matrix[:, i]\n",
    "\n",
    "            class_0 = np.where(ternary_code == 0)[0] # label as 1\n",
    "            class_1 = np.where(ternary_code == 1)[0] # label as 0\n",
    "\n",
    "            mask = np.isin(y, np.concatenate((class_0, class_1)))\n",
    "\n",
    "        \n",
    "\n",
    "            filtered_y = y[mask]\n",
    "            filtered_X = X[mask,:]\n",
    "\n",
    " \n",
    "            print(\"Training classifier {} with labels 0: {} and 1 :{}\".format(i, np.where(ternary_code==0), np.where(ternary_code==1)))\n",
    "            classifier = self.base_classifier.fit(filtered_X, filtered_y)\n",
    "            self.classifiers.append((ternary_code, classifier))\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_combinations = list(itertools.combinations(range(self.n_classes), 2))\n",
    "        predictions = []\n",
    "        for i in range(len(self.classifiers)):\n",
    "            ternary_code, classifier = self.classifiers[i]\n",
    "            binary_predictions = classifier.predict(X)\n",
    "            print(\"Predictions for classifier {} class{}and {} are {}\".format(i,class_combinations[i][0] ,class_combinations[i][1],np.unique(binary_predictions)))\n",
    "            predictions.append(binary_predictions)\n",
    "        return self.decode_labels(np.column_stack(predictions))\n",
    "\n",
    "    def generate_ovo_codes(self):\n",
    "        ecoc_matrix = np.ones((self.n_classes, self.n_classifiers), dtype=int)*-1\n",
    "        class_combinations = list(itertools.combinations(range(self.n_classes), 2))\n",
    "        for i, combination in enumerate(class_combinations):\n",
    "            class_i, class_j = combination\n",
    "            ecoc_matrix[class_i,i] = 1\n",
    "            ecoc_matrix[class_j,i] = 0\n",
    "        return ecoc_matrix\n",
    "\n",
    "    def decode_labels(self, predictions):\n",
    "        print (\"dicoding predictions: \", predictions)\n",
    "        print(self.ecoc_matrix)\n",
    "        min_distances = []\n",
    "        for pred_row in predictions:\n",
    "            hamming_distances = distance.cdist([pred_row], self.ecoc_matrix, metric='hamming')\n",
    "            min_distance_index = np.argmin(hamming_distances)\n",
    "            min_distances.append(min_distance_index)\n",
    "        return min_distances\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "# y_train = np.array([0, 1, 2, 1, 2, 0])\n",
    "# X_test = np.array([[13, 14], [15, 16], [17, 18]])\n",
    "\n",
    "# ecoc = TernaryECOC(3,LogisticRegression(),  3)\n",
    "# ecoc.train(X_train, y_train)\n",
    "# predictions = ecoc.predict(X_test)\n",
    "# print(predictions)\n",
    "\n",
    "\n",
    "\n",
    "# df=pd.read_csv('data/glass.csv',header=None)\n",
    "# X=df.iloc[:,:-1].values\n",
    "# y=df.iloc[:,-1].values\n",
    "\n",
    "X,y =get_X_y_data()\n",
    "# X,y = [[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[2],[2],[2]],[1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2]\n",
    "# X=np.array(X)\n",
    "# y=np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "n_classes = len(np.unique(y))\n",
    "n_classifiers = n_classes * (n_classes - 1) // 2\n",
    "ecoc = TernaryECOC(n_classes,LogisticRegression(),  n_classifiers)\n",
    "ecoc.train(X_train, y_train)\n",
    "predictions = ecoc.predict(X_test[4:5,:])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 2, 0, 2, 0, 2]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 1, 0, 1, 1, 2, 2, 0, 0, 1, 0, 0, 0, 2, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.22218546, -2.66822292,  1.26032575,  0.17933254]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2827836867.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[76], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    n_clusters_per_class\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def get_X_y_data():\n",
    "# Generate a synthetic classification dataset\n",
    "    X, y = make_classification(\n",
    "        n_samples=100,  # Number of samples\n",
    "    n_features=2,\n",
    "        n_informative=2, \n",
    "        n_redundant=0,\n",
    "        n_classes=3,\n",
    "        n_clusters_per_class\n",
    "        random_state=42  # Random seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # Extract the first feature from the dataset\n",
    "    first_feature = X[:, 0]\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.scatter(first_feature, range(len(first_feature)), c=y, cmap='viridis')\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('First Feature')\n",
    "    plt.ylabel('Index')\n",
    "    plt.title('Scatter Plot with Class Colors')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_classes(3) * n_clusters_per_class(1) must be smaller or equal 2**n_informative(1)=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_X_y_data()\n",
      "Cell \u001b[0;32mIn[52], line 6\u001b[0m, in \u001b[0;36mget_X_y_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_X_y_data\u001b[39m():\n\u001b[1;32m      5\u001b[0m \u001b[39m# Generate a synthetic classification dataset\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     X, y \u001b[39m=\u001b[39m make_classification(\n\u001b[1;32m      7\u001b[0m         n_samples\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,  \u001b[39m# Number of samples\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m     n_features\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m         n_informative\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[1;32m     10\u001b[0m         n_redundant\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m         n_classes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m         n_clusters_per_class\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m  \u001b[39m# Random seed for reproducibility\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[39m# Extract the first feature from the dataset\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     first_feature \u001b[39m=\u001b[39m X[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-MichiganStateUniversity/HAAL Lab/Projects/Conformed-ECOC/venv/lib/python3.11/site-packages/sklearn/datasets/_samples_generator.py:187\u001b[0m, in \u001b[0;36mmake_classification\u001b[0;34m(n_samples, n_features, n_informative, n_redundant, n_repeated, n_classes, n_clusters_per_class, weights, flip_y, class_sep, hypercube, shift, scale, shuffle, random_state)\u001b[0m\n\u001b[1;32m    185\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn_classes(\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) * n_clusters_per_class(\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m smaller or equal 2**n_informative(\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    188\u001b[0m         msg\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    189\u001b[0m             n_classes, n_clusters_per_class, n_informative, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mn_informative\n\u001b[1;32m    190\u001b[0m         )\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(weights) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [n_classes, n_classes \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]:\n",
      "\u001b[0;31mValueError\u001b[0m: n_classes(3) * n_clusters_per_class(1) must be smaller or equal 2**n_informative(1)=2"
     ]
    }
   ],
   "source": [
    "get_X_y_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
